{
 "metadata": {
  "name": "",
  "signature": "sha256:5472e3153ca8fadf18ae8ac147c9dda70ea24bc2deac83970b58c5e4e04c7ada"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Lab 8: Genetic Algorithm, Bayesian Model Averaging and Machine Learning\n",
      "\n",
      "AM207: Pavlos Protopapas, Harvard University\n",
      "\n",
      "March 28, 2014\n",
      "\n",
      "\n",
      "------\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)\n",
      "import seaborn as sns\n",
      "\n",
      "from IPython import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Goals\n",
      "This lab is going to be divided into three parts:\n",
      "\n",
      "1) Hint to the Genetic Algorithm Problem on the HW\n",
      "\n",
      "2) Machine Learning Introduction\n",
      "\n",
      "3) Bayesian Model Averaging\n",
      "\n",
      "# Hint for HW Problem 1\n",
      "\n",
      "## The Problem\n",
      "\n",
      "Consider a system consisting of 7 particles of type A and 12 particles of type B, which exist in a 2-dimensional (x, y) space. Two particles \n",
      "of type A interact among themselves with the Lennard-Jones (LJ) potential \n",
      "\n",
      "$$ V_{AA}(r_{12} )= \\epsilon_{A} \\left[  \\left( \\frac{\\sigma_{A}}{r_{12}} \\right) ^{12} - \\left( \\frac{\\sigma_{A}}{r_{12}} \\right) ^{6} \\right] $$\n",
      "\n",
      "where $r_{12} = \\sqrt{(x_1 - x_2)^2 + (y_1-y_2)^2 }$ is the distance between the particles at positions ($x_1$, $y_1$) and ($x_2$, $y_2$), and $\\sigma_A$, $\\epsilon_A$ are constants. Similarly particles of type B interact among themselves with a LJ potential \n",
      "$V_{BB}(r_{12})$ with different constants  $\\sigma_B$, $\\epsilon_B$  and particles of type A and type B interact with a LJ potential $V_{AB}(r_{12})$ with constants which are the geometric and arithmetic averages of the constants corresponding to $A - A$ and $B - B$ interactions, that is:\n",
      "\n",
      "$$ \\sigma_{AB} = \\sqrt{ \\sigma_A \\, \\sigma_B}, \\,\\,\\, \\epsilon_{AB} = \\frac{1}{2} (\\epsilon_A + \\epsilon_B).  $$\n",
      "\n",
      "Using the GA method, find the optimal configuration of the cluster\n",
      "of 19 $A$ and $B$ particles. Use the following values for the constants:\n",
      "\n",
      "$$ \\sigma_A=1.0, \\,\\, \\sigma_B=1.125,  \\,\\, \\epsilon_A=2.0,   \\,\\, \\epsilon_B = 1.5 $$\n",
      "\n",
      "\n",
      "## Suggestions\n",
      "\n",
      "As a first step, we examine the behavior of the interactions: a plot of the LJ potential reveals that it has an attractive part ($V_{LJ}(r) < 0$) and a repulsive part ($V_{LJ}(r) > 0$), with a minimum at $r_{\\text{min}} = 2^{1/6}\\sigma$ which is equal to $\\epsilon_{\\text{min}} = \u2212\\epsilon/4$, where $\\sigma$ and $\\epsilon$ are the length-scale and energy-scale parameters of the potential in its general form. The optimal configuration is the one with the lowest possible total energy, obtained by summing all the pair-wise interactions between atoms. These facts lead us to conclude that the atoms will be arranged so as to minimize the total energy in a structure that has as many pairs in the ideal distance as possible. The fitness function can then be taken to be simply the total energy with the opposite sign, so that total-energy values that are negative correspond to configurations of higher fitness. The atoms prefer to be close together but not too close, because beyond a certain point the interaction is repulsive. They also prefer to have as many neighbors as possible at the ideal distance $r_{\\text{min}}$. For a single pair of atoms this is easy to fulfill: just place the two atoms at the ideal distance which gives the lowest energy of the system, namely $\\epsilon_{\\text{min}}$. For three atoms, it is also easy to find the lowest energy structure, by simply\n",
      "placing the three atoms at the corners of an equilateral triangle of side\n",
      "equal to rmin. When more atoms are added to the system, things become\n",
      "more complicated because it is no longer possible to place all atoms in a\n",
      "structure so that all pair-wise distances are exactly equal to ideal distance,\n",
      "so some compromises must be made. In order to find reasonable candidates\n",
      "for low-energy structures, we can start with two atoms at the ideal distance\n",
      "and then place the rest of the atoms in some pattern around the original\n",
      "pair, and try to find the optimal structure. The equilateral triangle formed\n",
      "by three atoms suggests that a reasonable pattern may be one based on\n",
      "putting several such triangles together in a pattern that can keep growing.\n",
      "Based on this motif, we can then create individuals of a population that\n",
      "have the atoms in pair-wise optimal distances, and then move the atoms\n",
      "around this motif to see if we can obtain better structures. Notice that\n",
      "one atom can be held fixed and not allowed to move at all. The reason for\n",
      "this is that only the distances between atoms matter and not their absolute\n",
      "positions in space, so that shifting the whole structure (every single atom)\n",
      "by any given amount makes no difference to the total energy of the system;\n",
      "by fixing one atom in space permanently (we can consider it to be at the\n",
      "origin of the coordinate system) we make sure that any shifting of the whole\n",
      "structure, which is an irrelevant move as far as optimizing the total energy\n",
      "is concerned, is eliminated from the range of possible moves. By similar\n",
      "reasoning, a second atom can be held fixed at exactly the ideal distance\n",
      "from the first one; this eliminates any possibility of the whole structure\n",
      "rotating as a whole, which is also an irrelevant change in positions as far\n",
      "as optimizing the total energy is concerned. Therefore, there are only 17\n",
      "atoms that can be moved around to optimize the total energy of the system.\n",
      "Which two atoms should we fix? It seems like a reasonable choice to fix one\n",
      "atom of type A at the center (origin of the coordinate system) and a second\n",
      "atom also of type A at a distance $r_{\\text{min}}^{(A)} = 2^{1/6}\\sigma_A$ from the first, because these are the atoms with the strongest interaction $\\epsilon_{\\text{min}}^{(A)} = -\\epsilon_A/4 = -0.5$.  Having fixed those two atoms, we can then distribute the rest in a pattern, with distances as close as possible to the ideal distances (which depend on the type of atoms in each pair), and then create rules for changing the positions of atoms and their identity on the given pattern.  Several possible choices for the pattern should be considered to make sure that we covered enough of the space of possible configurations.\n",
      "\n",
      "\n",
      "#Introduction to Machine Learning\n",
      "Much of what we have been doing over the course of the semester so far actually fits under the umbrella category of \"machine learning.\"  Indeed, many of the stochastic optimization and sampling tools we cover in this course are considered part of the standard machine learning toolkit and will prove useful to you in future machine learning coursework, research, and practical applications. \n",
      "\n",
      "We will now go over some basic definitions and concepts in machine learning.  Many of you will already be familiar with these basic concepts but it is worthwhile going over them explicitly to make sure everyone is on the same page.\n",
      "\n",
      "There are two basic types of machine learning: **supervised and  unsupervised learning**.  In supervised learning, we receive a labeled set of input-output pairs (the ** training set**) from which we infer/learn a general mapping from inputs $x$ to outputs $y$.  A good example of a supervised learning problem is the O-ring problem you solved on HW2, where we received a set of input data (in this case the temperature) and a labeled output (whether failure occured or not). We used the training data to infer the values of relevant parameters and then updated to make forecasts for a new data point (the ** test set**). In unsupervised learning in contrast, there is no labeled training set.  We are simply given a set of input data from which we wish to find interesting patterns.  Many clustering algorithms, in which we wish to find groups of similar members without necessarily being given any predefined classes, is an example of unsupervised learning.\n",
      "\n",
      "Much of the work we will do in machine learning involves the task of ** classification** in which we attempt to learn a mapping from a set of inputs $x$ to outputs $y$ where $y \\in \\{1, ... C\\}$. ** Regression** can then be viewed simply as a classification problem with a continuous response variable $y$.\n",
      "\n",
      "Generally the first task in tackling any real-world problem in machine learning is ** model selection**.  We are often interested in a particular output variable, but the choice of which inputs are important in determining the output can be highly subjective.  In addition, deciding how exactly the output variables are related to a given set of input variables and how those inputs relate to each other can be very tricky, and in many cases there is no \"correct answer.\"  In selecting models, it is therefore important to remember the ** principle of parsimony**: always choose the simplest model that adequately describes your data.      Oftentimes, you'll find that a more complicated model may perform better in the training data, but when you move ** out-of-sample** to the test data, it will perform worse than a simpler model.  This problem is called ** overfitting**. \n",
      "\n",
      "In general our modeling goal should be to minimize the ** generalization rate**- the expected value of msiclassification rate when averaged over future (test) data.  Without having access of course to the future data, we rely on analysis of the **misclassification rate** (the percent of the training data misclassified by the model) to calibrate the model for good future performance.  Due to the problem of overfitting, however, it is generally not the case that a model that minimizes the misclassification rate will perform optimally out-of-sample.  There are many approaches that are commonly used to avoid overfitting. We will introduce (and expect you to become comfortable with) the technique of ** K-fold Cross Validation**.  The idea is to break the training data up into $k$ groups. We then train the model on $k-1$ group and test on the $k$-th group in a round robin fashion.  We then attempt to find the model that minimizes the average prediction error from all groups.  While selection of $k$ is of course subjective, setting $k=5$ is typically a good choice as this amounts to using 80% of our data for training and 20% for validation/prediction.\n",
      "\n",
      "Note, even in cases where we know with certainty that our data is truly high dimensional, there might only be several latent factors which describe most of the variability of the output variable in question and therefore complexity reduction can be achieved with minimal loss of accuracy.  Since more complex models are often associated with great computational cost to fitting and predicting from the model, it is generally useful to reduce dimensionality whenever possible.  Model exploration is therefore always important\n",
      "\n",
      "\n",
      "# Bayesian Model Averaging\n",
      "Bayesian model averaging (BMA) is typically employed to tackle problems where we do not know the true model underlying the system.  Intuitively, the idea is to test a variety of models (our model space) and let the data determine what *posterior* weight we put on each model.  Note that in BMA our goal is not to test a set of models and then choose the best (maximum likelihood) model, but rather to assign a weight to each model proportionate to our belief in that models validity (which itself is determined based on a combination of prior knowledge and the likelihood calculated from our data).  While it is possible that given enough data, BMA will converge to the single model maximum likelihood case, this is not necessarily the case.  Indeed, one of the key advantages of BMA is that it allows us to make robust inference in cases where the underlying system itself is not completely described by any individual model we've included in our model space.\n",
      "\n",
      "To formalize the idea of Bayesian model averaging we introduce the following notation.\n",
      "\n",
      "\n",
      "$Y$ is our quantity of interest (for our purposes future observation)\n",
      "\n",
      "$D$ is our data\n",
      "\n",
      "$\\mathcal{M}= \\{M_k, k = 1,2, ..., K\\}$ is the set of possible models.  We will consider here discrete model spaces of size $K$.\n",
      "\n",
      "$\\theta_k$ is a vector of model parameters.  Note that the $\\theta_k$ do not need to have the same dimensions and may or may not overlap.\n",
      "\n",
      "\n",
      "\n",
      "With this notation, our posterior probability on $Y$ given our data $D$ can be written as\n",
      "\\begin{equation}\n",
      " P(Y|D) = \\sum_{k=1}^K P(y|M_k, D) P(M_k|D)\n",
      " \\end{equation}\n",
      "In Equation 1 above $P(Y|M_k, D)$ can be thought of as our vanilla likelihood function for $Y$ given data and a particular model.  This probability can be easily evaluated using our standard techniques (either Bayesian or frequentist) for parameter fitting and forecasting.  The $P(M_k|D)$ represents our posterior model probability and can be somewhat trickier to evaluate.  Applying Bayes' rule we have\n",
      "\\begin{equation}\n",
      "P(M_k|D) = \\frac{P(D|M_k)P(M_k)}{\\sum_{i=1}^K P(D|M_i) P(M_i)}\n",
      "\\end{equation}\n",
      "where\n",
      "\\begin{equation}\n",
      "P(D|M_i) = \\int P(D|\\theta_i, M_i) P(\\theta_i | M_i) d\\theta_i\n",
      "\\end{equation}\n",
      "In Equation 2, $P(M_k)$ is simply our prior on model $M_k$ while $P(D|M_k)$ is the likelihood of $M_k$ given data $D$.  Equation 3 can be understood intuitively by noting that the probability of the data given model $i$ is just a weighted sum of the probability of the likelihood over the entire parameter space of the model.  Depending on the problem the integral in (3) can be difficult to evaluate.  We present an example problem later in this section showing how Monte Carlo techniques may be used to approximate the posterior model probability.\n",
      "\n",
      "Having specified the posterior model probability $P(M_k|D)$ we can now calculate the posterior mean and variance of $Y$.\n",
      "\n",
      "\\begin{align}\n",
      "E[Y|D] &= \\int y\\left(\\sum_{k=1}^K P(Y=y|M_k, D) P(M_k|D)\\right) dy \\\\\n",
      "&= \\sum_{k=1}^K \\left( \\int y P(Y=y|M_k, D) dy \\right) P(M_k|D) \\\\\n",
      "&= \\sum_{k=1}^K \\bar{Y}_k P(M_k |D) \n",
      "\\end{align}\n",
      "The posterior mean matches our intuition - it is just a sum of the model expectations weighted by the posterior probability of the model.  For variance now, we have\n",
      "\n",
      "\\begin{equation}\n",
      "Var[Y|D] = \\sum_{k=1}^K(Var[Y|D, M_k]+\\bar{Y}_k^2) P(M_k|D) - (E[Y|D])^2\n",
      "\\end{equation}\n",
      "\n",
      "\\subsection*{Model Space Selection}\n",
      "Thus far we have assumed that the model space $\\mathcal{M}$ is given.  Though technically $\\mathcal{M}$ is  infinite, BMA tends to be constrained to handle discrete model spaces.  Even after ignoring those models with zero prior probability, it can be infeasible to consider all possible models. One approach to limit the model space is called {\\bf Occam's window}. In model space selection there are two aspects we wish consider: 1) the model's performance and 2) its complexity.  We formalize these criteria as follows:\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{A}' &= \\left\\{ M_k: \\frac{\\max_I\\{P(M_I|D)\\}}{P(M_k|D)} \\le O_L \\right\\}\\\\\n",
      "\\mathcal{B} &= \\left\\{ M_k: \\exists M_I \\in \\mathcal{A}', M_I \\subset M_k, \\frac{P(M_I|D)}{P(M_k|D)}>O_R \\right\\}\n",
      "\\end{align}\n",
      "Our model space then is $\\mathcal{A} = \\mathcal{A}' \\backslash \\mathcal{B} $. Though choices for the cutoffs $O_L$ and $O_R$ can vary with application, a reasonable choice might be to set $O_L \\le \\frac{1}{20}$ and $O_R =1$.  In other words we ignore all models who are $95\\%$ less likely than the best model in our model space as well as those for which there exists a simpler and equally effective model in the model space. \n",
      "\n",
      "## Example Model\n",
      "To develop a deeper understanding of BMA and its applications, we derived a contrived example problem to which we applied BMA.  We generated data from the following model: \n",
      "$$ y(x)= 0.05x^2+3x + \\epsilon_t $$\n",
      "where the $\\epsilon_t$ are IID noise distributed as $\\epsilon_t \\sim N(0,1)$.  \n",
      "\n",
      "While the underlying model here is quadratic, from looking at Figure 1 it is not immediately clear what model to fit to the data.  We let our model space consist of the following 3 models\n",
      "\n",
      "*Linear*: $\\theta_1 x + \\epsilon_t$\n",
      "\n",
      "*Quadratic*: $\\theta_1 x^2+\\theta_2 x + \\epsilon_t$\n",
      "\n",
      "*Exponential*: $\\theta_1 e^{\\theta_2 x}+\\epsilon_t$\n",
      "\n",
      "\n",
      "We wish to calculate the expected value of $y(11)$. For this example we assume that the noise standard deviation is given. Our first task is to calculate the likelihood of each model.  From Equation 3) recall\n",
      "$$P(D|M_i) = \\int P(D|\\theta_i, M_i) P(\\theta_i | M_i) d\\theta_i$$\n",
      "For simplicity we will assume that $P(\\theta_i|M_i)$ is uniform on the range $[-15,15]$ (linear) or $[-15,15]\\times [-15,15]$ (quadratic, exponential).  Note that $P(D|\\theta_i, M_i)$ here is simply the multivariate normal probability density function with mean vector 0 and identity covariance matrix evaluated at the residual vector $D-\\hat{D}(\\theta)$.  To approximate the integral of this probability over all possible values of $\\theta$, we employ a combination of slice sampler Markov Chain Monte Carlo and Monte Carlo integration.\n",
      "\n",
      "To determine a finite range of $\\theta$ over which to integrate in each model, we sample $\\theta$ from the above distribution 15,000 times and set our integration bounds equal to the range of $\\theta$ observed in this sample.  To approximate the integral we bound the desired integral by a rectangle (rectangular prism for the quadratic and exponential models), where we set the height to be  1.1 times the maximum value of the above PDF achieved with our sampled $\\theta$.  Applying vanilla Monte Carlo simulation, we draw points uniformly at random ($10^5$ draws here) from the rectangle and record the fraction of points that lie under the given PDF.  The integral is then approximated as the volume of the bounding prism times this fraction.\n",
      "\n",
      "Having calculated $P(D|M_i)$ we can calculate the posterior model probability $P(M_k|D)$ according to Equation 2.  For our data we find that the posterior probability of the linear model was roughly $15\\%$, that of the quadratic model was $85\\%$, while that of the exponential model was $5.8\\times10^{-11}$.  BMA appears to have successfully identified the quadratic model as the most probably model here, but still assigns a non-trivial probability to the linear model, which is reasonable given the small coefficient of the $x^2$ term. \n",
      "\n",
      "To solve for $E[y(11)|D]$ finally, we use our sampled values of $\\theta$ to identify the maximum a posteriori (MAP) estimates of $\\theta$ for each model, which allow us to estimate $E[y(11)|D, M_k]$.  Combining these forecasts as in the equation above, we find that our final forecast is 44.4 which is quite close to the true underlying value at $x=11$ of 45.1.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}