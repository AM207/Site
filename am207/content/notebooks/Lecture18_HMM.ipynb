{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Lecture 19: Hidden Markov Models\n",
      "\n",
      "AM207: Pavlos Protopapas, Harvard University\n",
      "\n",
      "April 8, 2014\n",
      "\n",
      "\n",
      "------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A simple and effective method of representing observations in the real world is as noisy reflections of a 'true' state. For example, the stock price can be considered a noisy reflection of the actual value of the company. Or, the way you feel on a certain day can be thought of as a noisy relfection of whether of not you have a fever (For instance, sometimes you may feel cold or dizzy even if you aren't sick).\n",
      "\n",
      "Modeling a time series of events in such a way is called a Hidden Markov Model (HMM). There are the observed states and the latent states that determine them. There is some emmission probability of observed states given the latent state at each time step. There is a also a transition probability between latent states. Consider the below illustration, with x being the latent state and y being the observed state at each time step. The arrows indicate transitions which are decided by a transition probability distribution.\n",
      "\n",
      "<img src=\"files/hmm.png\" height=\"150px\">\n",
      "\n",
      "There are three fundamental problems for HMMs:\n",
      "\n",
      "1. Evaluation: Given the model parameters and observed data, calculate the likelihood of the data.\n",
      "\n",
      "2. Decoding: Given the model parameters and observed data, estimate the optimal sequence of hidden states.\n",
      "\n",
      "3. Learning: Given just the observed data, estimate the model parameters."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of the evaluation problem is to return the probability of a given sequence of observations given a sequence of latent states. Let us first introduce the toy problem. In this problem, we consider a person's latent state of health and their apparent symptoms. In this instance, a person is either Healthy or has a Fever, and the possible symptoms are feeling normal, cold, and dizzy. There is no deterministic relationship between the latent and observed states, so we need probabilistic approach to solving the problem of evaluation. \n",
      "\n",
      "<img src=\"files/hmm1.png\" height=\"500px\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states = ('Healthy', 'Fever')\n",
      "end_state = 'E'\n",
      " \n",
      "observations = ('cold', 'normal', 'normal')\n",
      " \n",
      "start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n",
      "\n",
      "transition_probability = {\n",
      "   'Healthy' : {'Healthy': 0.69, 'Fever': 0.3, 'E': 0.01},\n",
      "   'Fever' : {'Healthy': 0.4, 'Fever': 0.59, 'E': 0.01},\n",
      "   }\n",
      " \n",
      "emission_probability = {\n",
      "   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
      "   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},\n",
      "   }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Forward-Backward Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the model with transition probabilities in place, we need a way to determine the likelihood of a certain series of observations. In the above example, the observations are feeling cold the first day and normal the next two. We use these observations to return a ????\n",
      "\n",
      "In the first pass, the forward\u2013backward algorithm computes a set of forward probabilities which provide, for all $k \\in \\{1, \\dots, t\\}$, the probability of ending up in any particular state given the first $k$ observations in the sequence, i.e. $P(X_k\\ |\\  Y_{1:k})$. In the second pass, the algorithm computes a set of backward probabilities which provide the probability of observing the remaining observations given any starting point $k$, i.e. $P(Y_{k+1:t}\\ |\\ X_k)$. These two sets of probability distributions can then be combined to obtain the distribution over states at any specific point in time given the entire observation sequence:\n",
      "\n",
      "$P(X_k\\ |\\ Y_{1:t}) = P(X_k\\ |\\ Y_{1:k}, Y_{k+1:t}) \\propto P(Y_{k+1:t}\\ |\\ X_k) P(X_k\\ |\\ Y_{1:k})$\n",
      "\n",
      "The last step follows from an application of Bayes' rule and the conditional independence of $Y_{k+1:t}$ and $Y_{1:k}$ given $X_k$.\n",
      "\n",
      "As outlined above, the algorithm involves three steps:\n",
      "\n",
      "1. computing forward probabilities\n",
      "\n",
      "2. computing backward probabilities\n",
      "\n",
      "3. computing smoothed values.\n",
      "\n",
      "The forward and backward steps may also be called \"forward message pass\" and \"backward message pass\" - these terms are due to the message-passing used in general belief propagation approaches. At each single observation in the sequence, probabilities to be used for calculations at the next observation are computed. The smoothing step can be calculated simultaneously during the backward pass. This step allows the algorithm to take into account any past observations of output for computing more accurate results.\n",
      "\n",
      "The forward\u2013backward algorithm can be used to find the most likely state for any point in time. It cannot, however, be used to find the most likely sequence of states (see Decoding, and specifically the Viterbi algorithm)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fwd_bkw(x, states, a_0, a, e, end_st):\n",
      "    L = len(x)\n",
      " \n",
      "    fwd = []\n",
      "    f_prev = {}\n",
      "    # forward part of the algorithm\n",
      "    for i, x_i in enumerate(x):\n",
      "        f_curr = {}\n",
      "        for st in states:\n",
      "            if i == 0:\n",
      "                # base case for the forward part\n",
      "                prev_f_sum = a_0[st]\n",
      "            else:\n",
      "                prev_f_sum = sum(f_prev[k]*a[k][st] for k in states)\n",
      " \n",
      "            f_curr[st] = e[st][x_i] * prev_f_sum\n",
      " \n",
      "        fwd.append(f_curr)\n",
      "        f_prev = f_curr\n",
      " \n",
      "    p_fwd = sum(f_curr[k]*a[k][end_st] for k in states)\n",
      " \n",
      "    bkw = []\n",
      "    b_prev = {}\n",
      "    # backward part of the algorithm\n",
      "    for i, x_i_plus in enumerate(reversed(x[1:]+(None,))):\n",
      "        b_curr = {}\n",
      "        for st in states:\n",
      "            if i == 0:\n",
      "                # base case for backward part\n",
      "                b_curr[st] = a[st][end_st]\n",
      "            else:\n",
      "                b_curr[st] = sum(a[st][l]*e[l][x_i_plus]*b_prev[l] for l in states)\n",
      " \n",
      "        bkw.insert(0,b_curr)\n",
      "        b_prev = b_curr\n",
      " \n",
      "    p_bkw = sum(a_0[l] * e[l][x[0]] * b_curr[l] for l in states)\n",
      " \n",
      "    # merging the two parts\n",
      "    posterior = []\n",
      "    for i in range(L):\n",
      "        posterior.append({st: fwd[i][st]*bkw[i][st]/p_fwd for st in states})\n",
      " \n",
      "    assert p_fwd == p_bkw\n",
      "    return fwd, bkw, posterior"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def example():\n",
      "    return fwd_bkw(observations,\n",
      "                   states,\n",
      "                   start_probability,\n",
      "                   transition_probability,\n",
      "                   emission_probability,\n",
      "                   end_state)\n",
      " \n",
      "fwd, bkw, posterior = example()\n",
      "print posterior"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'Healthy': 0.7523637371047066, 'Fever': 0.24763626289529342}, {'Healthy': 0.9154595401170142, 'Fever': 0.08454045988298578}, {'Healthy': 0.907504985311503, 'Fever': 0.09249501468849691}]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Decoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decoding an HMM is answering the question: \"Given the transition model, what is the most likely sequence of latent states that could have produced the observed states?\" This is different than the evaulation problem because it predicts an entire sequence rather than giving a probability distribution over the latent states for each time step. \n",
      "\n",
      "A naive way of solving the decoding problem is simply by exhausting evaulation, i.e. to list all possible sequences of hidden states and finding the probability of the observed sequence for each of the combinations. The most probable sequence of hidden states is that combination that maximises P(observed sequence | hidden state sequence).\n",
      "\n",
      "This approach is viable, but to find the most probable sequence by exhaustively calculating each combination is computationally expensive. As with the forward algorithm, we can use the time invariance of the probabilities to reduce the complexity of the calculation.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Viterbi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most popular algorithm for the HMM  decoding problem is the Viterbi algorithm, a dynamic programming solution (for the most likely set of hidden states). Viterbi recursively finds the most probable sequence of hidden states given an observation sequence and a HMM. It does it by first defining the partial probability  $d$, which is the probability of reaching a particular intermediate state in the time sequence. These partial probabilities differ from those calculated in the forward algorithm since they represent the probability of the most probable path to a state at time $t$, and not a total. Thus $d (s_i,t)$ is the maximum probability of all sequences ending at state $s_i$ at time $t$, and the partial best path is the sequence which achieves this maximal probability. Thus, to calculate the partial probabilities $d$ at time $t$, we only need to know the probabilties $d$ at time $t-1$. \n",
      "\n",
      "$$d(s_i, t) = \\max_{\\forall j}(d(s_j, t-1) \\times P(s_j,s_i))$$\n",
      "\n",
      "In the below implementation, the accuracy is assessed by generating the hidden and observed states ourselves and then comparing them against the Viterbi proposal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a slightly more realistic version of the 'Healthy' vs. 'Fever' model is outlined and then generated\n",
      "import random\n",
      "states = ('Healthy', 'Fever')\n",
      "observations = ('normal', 'cold', 'dizzy')\n",
      "start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n",
      "\n",
      "transition_probability = {\n",
      "   'Healthy' : {'Healthy': 0.8, 'Fever': 0.2},\n",
      "   'Fever' : {'Healthy': 0.4, 'Fever': 0.6}\n",
      "   }\n",
      "\n",
      "emission_probability = {\n",
      "   'Healthy' : {'normal': 0.7, 'cold': 0.2, 'dizzy': 0.1},\n",
      "   'Fever' : {'normal': 0.1, 'cold': 0.4, 'dizzy': 0.5}\n",
      "   }\n",
      "\n",
      "# A random HMM is generated using the probability matricies defined above\n",
      "# Both the latent and visible states are generated in order to later assess the accuracy of our algorithms\n",
      "N = 100\n",
      "hidden = []\n",
      "visible = []\n",
      "# generate observations\n",
      "if random.random() < start_probability[states[0]]:\n",
      "    hidden.append(states[0])\n",
      "else:\n",
      "    hidden.append(states[1])\n",
      "\n",
      "for i in xrange(N):\n",
      "    current_state = hidden[i]\n",
      "    if random.random() < transition_probability[current_state][states[0]]:\n",
      "        hidden.append(states[0])\n",
      "    else:\n",
      "        hidden.append(states[1])\n",
      "    r = random.random()\n",
      "    prev = 0\n",
      "    for observation in observations:\n",
      "        prev += emission_probability[current_state][observation]\n",
      "        if r < prev:\n",
      "            visible.append(observation)\n",
      "            break\n",
      "\n",
      "# fixes issue of creating extra hidden state\n",
      "hidden.pop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "'Fever'"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_dptable(V):\n",
      "    s = \"    \" + \" \".join((\"%7d\" % i) for i in range(len(V))) + \"\\n\"\n",
      "    for y in V[0]:\n",
      "        s += \"%.5s: \" % y\n",
      "        s += \" \".join(\"%.7s\" % (\"%f\" % v[y]) for v in V)\n",
      "        s += \"\\n\"\n",
      "    print(s)\n",
      "\n",
      "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
      "    V = [{}]\n",
      "    path = {}\n",
      " \n",
      "    # Initialize base cases (t == 0)\n",
      "    for y in states:\n",
      "        V[0][y] = start_p[y] * emit_p[y][obs[0]]\n",
      "        path[y] = [y]\n",
      " \n",
      "    # Run Viterbi for t > 0\n",
      "    for t in range(1, len(obs)):\n",
      "        V.append({})\n",
      "        newpath = {}\n",
      " \n",
      "        for y in states:\n",
      "            (prob, state) = max((V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states)\n",
      "            V[t][y] = prob\n",
      "            newpath[y] = path[state] + [y]\n",
      " \n",
      "        # Don't need to remember the old paths\n",
      "        path = newpath\n",
      "     \n",
      "    #print_dptable(V)\n",
      "    (prob, state) = max((V[t][y], y) for y in states)\n",
      "    return (prob, path[state])\n",
      "\n",
      "# input the generated markov model\n",
      "def example():\n",
      "    return viterbi(visible,\n",
      "                   states,\n",
      "                   start_probability,\n",
      "                   transition_probability,\n",
      "                   emission_probability)\n",
      "\n",
      "(prob, p_hidden) = example()\n",
      "\n",
      "# assess accuracy of the HMM model\n",
      "wrong= 0\n",
      "for i in range(len(hidden)):\n",
      "    if hidden[i] != p_hidden[i]:\n",
      "        wrong = wrong + 1\n",
      "print \"accuracy: \" + str(1-float(wrong)/N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy: 0.83\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The learning problem with HMMs is about estimating the maximum liklihood model which generated the observed sequence. \n",
      "\n",
      "One popular method of doing this the Baum-Welch algorithm which is basically an EM forward-backward implementation which alternates between estimating the most likely hidden states, and the most likely model that produced them. While we won't go into detail regarding the details, there exists an sklearn implementation which we can use for a fun example below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stock Price as a HMM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you have seen a trivial example of the use of HMM's to infer the underlying state of a time-series of observations, consider a more complicated user-case: the stock market. Below is example code for using sklearn's GaussianHMM module to infer a set number of hidden states from Google stock data. What is underlying stock price? A non-cynical investor would say the company's value. What the below HMM implementation does is classify each day into one of, in this case 3, latent states. These states could be thought of as: \"Google had a good day\", \"Google had a neutral day\", and \"Google had a bad day\". "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "from matplotlib.finance import quotes_historical_yahoo\n",
      "from matplotlib.dates import YearLocator, MonthLocator, DateFormatter\n",
      "from sklearn.hmm import GaussianHMM\n",
      "\n",
      "###############################################################################\n",
      "# Downloading the data\n",
      "date1 = datetime.date(1995, 1, 1)  # start date\n",
      "date2 = datetime.date(2012, 1, 6)  # end date\n",
      "# get quotes from yahoo finance\n",
      "quotes = quotes_historical_yahoo(\"GOOG\", date1, date2)\n",
      "if len(quotes) == 0:\n",
      "    raise SystemExit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "HTTPError",
       "evalue": "HTTP Error 404: Not Found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-16-bfc36dcc9574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdate2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2012\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# end date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# get quotes from yahoo finance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mquotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquotes_historical_yahoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquotes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/lib/python2.7/site-packages/matplotlib/finance.pyc\u001b[0m in \u001b[0;36mquotes_historical_yahoo\u001b[0;34m(ticker, date1, date2, asobject, adjusted, cachename)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m#    warnings.warn(\"Recommend changing to asobject=None\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_historical_yahoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/lib/python2.7/site-packages/matplotlib/finance.pyc\u001b[0m in \u001b[0;36mfetch_historical_yahoo\u001b[0;34m(ticker, date1, date2, cachename, dividends)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mmkdirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0murlfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/python.app/Contents/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_opener\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0m_opener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/python.app/Contents/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/python.app/Contents/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 523\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/python.app/Contents/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/python.app/Contents/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/pprotopapas/anaconda/python.app/Contents/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# unpack quotes\n",
      "dates = np.array([q[0] for q in quotes], dtype=int)\n",
      "close_v = np.array([q[2] for q in quotes])\n",
      "volume = np.array([q[2] for q in quotes])[1:]\n",
      "\n",
      "# take diff of close value\n",
      "# this makes len(diff) = len(close_t) - 1\n",
      "# therefore, others quantity also need to be shifted\n",
      "diff = close_v[1:] - close_v[:-1]\n",
      "dates = dates[1:]\n",
      "close_v = close_v[1:]\n",
      "\n",
      "# pack diff and volume for training\n",
      "X = np.column_stack([diff, volume])\n",
      "\n",
      "###############################################################################\n",
      "# Run Gaussian HMM\n",
      "print \"fitting to HMM and decoding ...\",\n",
      "n_components = 3\n",
      "\n",
      "# make an HMM instance and execute fit\n",
      "model = GaussianHMM(n_components, \"diag\")\n",
      "model.fit([X], n_iter=1000)\n",
      "\n",
      "# predict the optimal sequence of internal hidden state\n",
      "hidden_states = model.predict(X)\n",
      "\n",
      "print \"done\\n\"\n",
      "\n",
      "###############################################################################\n",
      "# print trained parameters and plot\n",
      "print \"Transition matrix\"\n",
      "print model.transmat_\n",
      "print \"\"\n",
      "\n",
      "print \"means and vars of each hidden state\"\n",
      "for i in xrange(n_components):\n",
      "    print \"%dth hidden state\" % i\n",
      "    print \"mean = \", model.means_[i]\n",
      "    print \"var = \", np.diag(model.covars_[i])\n",
      "    print \"\"\n",
      "\n",
      "years = YearLocator()   # every year\n",
      "months = MonthLocator()  # every month\n",
      "yearsFmt = DateFormatter('%Y')\n",
      "fig = figure()\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "for i in xrange(n_components):\n",
      "    # use fancy indexing to plot data in each state\n",
      "    idx = (hidden_states == i)\n",
      "    ax.plot_date(dates[idx], close_v[idx], 'o', label=\"%dth hidden state\" % i)\n",
      "ax.legend()\n",
      "\n",
      "# format the ticks\n",
      "ax.xaxis.set_major_locator(years)\n",
      "ax.xaxis.set_major_formatter(yearsFmt)\n",
      "ax.xaxis.set_minor_locator(months)\n",
      "ax.autoscale_view()\n",
      "\n",
      "# format the coords message box\n",
      "ax.fmt_xdata = DateFormatter('%Y-%m-%d')\n",
      "ax.fmt_ydata = lambda x: '$%1.2f' % x\n",
      "ax.grid(True)\n",
      "\n",
      "fig.autofmt_xdate()\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Kalman Filter"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kalman filters solve the learning problem like Bauch-Welch except that the hidden state variables are seen in a continuous space, rather than a discrete one. Additionally, the hidden Markov model can represent an arbitrary distribution for the next value of the state variables, in contrast to the Gaussian noise model that is used for the Kalman filter.\n",
      "\n",
      "They are modelled on a Markov chain built on linear operators perturbed by errors that may include Gaussian noise. The state of the system is represented as a vector of real numbers. At each discrete time increment, a linear operator is applied to the state to generate the new state, with some noise mixed in, and optionally some information from the controls on the system if they are known. Then, another linear operator mixed with more noise generates the observed outputs from the true (\"hidden\") state.\n",
      "\n",
      "In order to use the Kalman filter to estimate the internal state of a process given only a sequence of noisy observations, one must model the process in accordance with the framework of the Kalman filter. This means specifying the following matrices: $Fk$, the state-transition model; Hk, the observation model; $Qk$, the covariance of the process noise; $Rk$, the covariance of the observation noise; and sometimes $Bk$, the control-input model, for each time-step, $k$, as described below.\n",
      "\n",
      "\n",
      "Model underlying the Kalman filter. Squares represent matrices. Ellipses represent multivariate normal distributions (with the mean and covariance matrix enclosed). Unenclosed values are vectors. In the simple case, the various matrices are constant with time, and thus the subscripts are dropped, but the Kalman filter allows any of them to change each time step.\n",
      "The Kalman filter model assumes the true state at time $k$ is evolved from the state at $k\u22121$ according to\n",
      "\n",
      "$$x_{k} = F_{k} x_{k-1} + B_{k} u_{k} + w_{k} $$\n",
      "\n",
      "where\n",
      "\n",
      "$Fk$ is the state transition model which is applied to the previous state $xk\u22121$;\n",
      "\n",
      "$Bk$ is the control-input model which is applied to the control vector $uk$;\n",
      "\n",
      "$wk$ is the process noise which is assumed to be drawn from a zero mean multivariate normal distribution with covariance $Qk$.\n",
      "\n",
      "$w_{k} \\sim N(0, Q_k) $\n",
      "\n",
      "At time k an observation (or measurement) zk of the true state xk is made according to\n",
      "\n",
      "$z_{k} = H_{k} x_{k} + v_{k}$\n",
      "\n",
      "where $Hk$ is the observation model which maps the true state space into the observed space and $vk$ is the observation noise which is assumed to be zero mean Gaussian white noise with covariance $Rk$.\n",
      "$v_{k} \\sim N(0, R_k) $\n",
      "\n",
      "The initial state, and the noise vectors at each step ${x0, w1, ..., wk, v1 ... vk}$ are all assumed to be mutually independent.\n",
      "\n",
      "Many real dynamical systems do not exactly fit this model. In fact, unmodelled dynamics can seriously degrade the filter performance, even when it was supposed to work with unknown stochastic signals as inputs. The reason for this is that the effect of unmodelled dynamics depends on the input, and, therefore, can bring the estimation algorithm to instability (it diverges). On the other hand, independent white noise signals will not make the algorithm diverge. The problem of separating between measurement noise and unmodelled dynamics is a difficult one and is treated in control theory under the framework of robust control."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Kalman filter example demo in Python\n",
      "\n",
      "# A Python implementation of the example given in pages 11-15 of \"An\n",
      "# Introduction to the Kalman Filter\" by Greg Welch and Gary Bishop,\n",
      "# University of North Carolina at Chapel Hill, Department of Computer\n",
      "# Science, TR 95-041,\n",
      "# http://www.cs.unc.edu/~welch/kalman/kalmanIntro.html\n",
      "\n",
      "# by Andrew D. Straw\n",
      "\n",
      "import numpy\n",
      "import pylab\n",
      "# intial parameters\n",
      "sz=50\n",
      "x = -.5 # truth value\n",
      "z = numpy.random.normal(x,0.1,size=sz) # observations (normal about x, sigma=0.1)\n",
      "\n",
      "Q = 1e-5 # process variance\n",
      "\n",
      "# allocate space for arrays\n",
      "xhat=numpy.zeros(sz)      # a posteri estimate of x\n",
      "P=numpy.zeros(sz)         # a posteri error estimate\n",
      "xhatminus=numpy.zeros(sz) # a priori estimate of x\n",
      "Pminus=numpy.zeros(sz)    # a priori error estimate\n",
      "K=numpy.zeros(sz)         # gain or blending factor\n",
      "\n",
      "R = 0.1**2 # estimate of measurement variance, change to see effect\n",
      "\n",
      "# intial guesses\n",
      "xhat[0] = 0.0\n",
      "P[0] = 1.0\n",
      "\n",
      "for k in range(1,sz):\n",
      "    # time update\n",
      "    xhatminus[k] = xhat[k-1]\n",
      "    Pminus[k] = P[k-1]+Q\n",
      "\n",
      "    # measurement update\n",
      "    K[k] = Pminus[k]/( Pminus[k]+R )\n",
      "    xhat[k] = xhatminus[k]+K[k]*(z[k]-xhatminus[k])\n",
      "    P[k] = (1-K[k])*Pminus[k]\n",
      "\n",
      "pylab.figure()\n",
      "pylab.plot(z,'k+',label='noisy measurements')\n",
      "pylab.plot(xhat,'b-',label='a posteri estimate')\n",
      "pylab.axhline(x,color='g',label='truth value')\n",
      "pylab.legend()\n",
      "pylab.xlabel('Iteration')\n",
      "pylab.ylabel('Voltage')\n",
      "\n",
      "pylab.figure()\n",
      "valid_iter = range(1,n_iter) # Pminus not valid at step 0\n",
      "pylab.plot(valid_iter,Pminus[valid_iter],label='a priori error estimate')\n",
      "pylab.xlabel('Iteration')\n",
      "pylab.ylabel('$(Voltage)^2$')\n",
      "pylab.setp(pylab.gca(),'ylim',[0,.01])\n",
      "pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Baysian Filtering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In many applications of HMMs, the observed data arrives sequentially over time leading to\n",
      "online inference problems such as Bayesian filtering. In this case, we want to estimate the\n",
      "posterior distribution $p(x_t|y_1,\\ldots, y_t)$. This is the distribution over  just the current state\n",
      "at time $t$ given all measurements up to and including time $t$. Note that we are not asking\n",
      "here about the full history of the system state, but only the latest state. If the full history\n",
      "is needed, it might be best to use MCMC methods. In the filtering task, SMC methods can\n",
      "be applied efficiently because the results of the inference procedure at time $t$ can be re-used\n",
      "again at time $t + 1$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Particle Filters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the SMC approach to filtering, the posterior distribution over the current latent state\n",
      "is based on an approximation from a finite set of samples. Particle filtering is one form of\n",
      "SMC. In particle filters, each particle represents a sample or hypothesis about the current\n",
      "latent state. As the number of particles increases, the approximation improves and starts to\n",
      "resemble to the exact Bayesian posterior. Although a low number of particles is not desirable\n",
      "for engineering applications, as it can lead to poor approximations, they can be very useful\n",
      "to explain suboptimal human performance in a number of sequential tasks. Note that particle filtering does not refer to a particular algorithm that rather a general approach \u2013 there are many different methods to implement particle filters.\n",
      "\n",
      "A particularly attractive feature of particle filters is that no memory of previous states\n",
      "is needed. All the information needed for the filtering task is contained in a fixed set of\n",
      "particles leading to a constant memory demand over time. However, when applying particle\n",
      "filters to high-dimensional problems, a very large number of particles might be needed to\n",
      "achieve good performance. This can in some cases lead to very slow performance, especially\n",
      "when overly simplistic particle filters are used."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sample Importance Resampling (SIR)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the most basic particle filters is Sampling Importance Resampling (SIR). It is based\n",
      "on the idea of importance sampling where a distribution is approximated by a set of samples\n",
      "weighted by the density of the target function such that the dense regions of the posterior\n",
      "are more heavily weighted than the tails. In the SIR filter, we always work with a fixed set\n",
      "of $M$ particles. To introduce notation, let $x^i_t$ represent the hypothesis about the latent state\n",
      "of the ith particle at time $t$. Across particles, we can approximate the distribution of the\n",
      "filtered estimate at time $t$ by the set of particles $\\{x^i_t\\}$. If a point estimate is needed for the latent state, we can take the mode or mean of this distribution. \n",
      "\n",
      "The SIR particle filter as described below is desirable for its simplicity. It only requires that one can sample from the\n",
      "state transition distribution $p(x_t|x_{t-1})$ and can evaluate the likelihood \n",
      "$p(y_t|x_t)$.\n",
      "\n",
      "We initialize the particle filter by sampling each particle from the prior distribution\n",
      "$p(x_0)$. At each iteration, we then evolve the set of particles from the last generation. First,\n",
      "we sample for each particle from the last generation a candidate hypothesis for the current\n",
      "latent state. Therefore, each particle leads to a projection of the hidden state to the current\n",
      "time step. We then calculate the likelihood of the current observation $y_t$ under each particle.\n",
      "This likelihood serves as the importance weight. We then normalize the importance weights\n",
      "across particles. In the second step, we evolve the set of particles from the last generation\n",
      "by sampling, with replacement, from the candidate hypotheses weighted by the importance\n",
      "weights. Therefore, candidate hypotheses that make the current observations likely lead to\n",
      "higher importance weights and are more likely to be sampled. This resampling continues\n",
      "until $M$ new particles are generated.\n",
      "\n",
      "Note that in the resampling step, the same proposal can be sampled multiple times. Also,\n",
      "a particular proposal might not be sampled at all. Therefore, each particle can lead to a\n",
      "varying number of \u201co\ufb00spring\u201d depending on the proximity of particle to the region in state\n",
      "space that best explains the current observation \u2013 bad particles die out and are taken over\n",
      "by particles that do (at least for the current time step) explain the data.\n",
      "In this particular version of SIR, the proposal distribution equals the state transition\n",
      "distribution. This version of SIR is also known as the bootstrap or condensation algorithm.\n",
      "It is important to realize that this choice of proposal distribution can lead to problems with\n",
      "high-dimensional cases. This is because in the first step of the SIR approach, we sample\n",
      "proposals from the state transition distribution without taking the current observation into\n",
      "account. The current observation is taken into account only in the second step, by weighting\n",
      "each particle by the likelihood. Therefore, we are blindly proposing how the current hidden\n",
      "state evolves from the last state and then correct only afterwards for any unlikely proposals. \n",
      "In some cases, this leads to a situation where none of the candidate proposals come near\n",
      "the regions in the state space that can adequately explain the current observation. In other\n",
      "versions of SIR particle filters, more complex proposal distributions are used that do take\n",
      "the current observation into account (in the proposal generation). Also, some authors have\n",
      "proposed methods such as unscented Kalman filters to deal with this problem. The number of\n",
      "particles, $M$ was set to 100. The bottom panel shows the distribution over particles for each\n",
      "iteration. The middle panel shows the mode of the particles to create a point estimate for\n",
      "the latent state at each time t. Importantly, remember that in the filtering task, a filtering\n",
      "estimate at time t implies that all data up to and including time t has been observed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's do an example with Pavlos smiling. Pavlos can either be happy or sad. Pavlos switches moods about once every ten seconds (typical).\n",
      "#If Pavlos is happy, there is a .8 chance that he will smile. If he is sad, there is only .2 chance of him smiling.\n",
      "\n",
      "#probability that Pavlos will switch moods this second\n",
      "a=.1\n",
      "\n",
      "#probability that Pavlos will smile, given that he's happy (1-b if he's not happy)\n",
      "b=.8\n",
      "\n",
      "#we can thus generate an set of observations based on this model\n",
      "Ttotal=200\n",
      "\n",
      "#hidden states\n",
      "xs = zeros((1,Ttotal))[0]\n",
      "xs[0] = randint(2)\n",
      " \n",
      "#observed states\n",
      "y = zeros((1,Ttotal))[0]\n",
      "\n",
      "for t in range(1,Ttotal):\n",
      "    if rand() < a:\n",
      "        xs[t] = 1-xs[t-1]\n",
      "    else:\n",
      "        xs[t] = xs[t-1]\n",
      "    if xs[t]:\n",
      "        if rand() < b:\n",
      "            y[t] = 1\n",
      "        else:\n",
      "            y[t] = 0\n",
      "    else:\n",
      "        if rand() < 1-b:\n",
      "            y[t] = 1\n",
      "        else:\n",
      "            y[t] = 0\n",
      "\n",
      "plot([i for i in range(Ttotal)], xs)\n",
      "ylim([-1,2])\n",
      "plot([i for i in range(Ttotal)], y)\n",
      "legend(['happy?', 'smiling?'])\n",
      "xlabel('time')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#number of particles\n",
      "M=100\n",
      "\n",
      "xp=ones((M,Ttotal))\n",
      "x= randint(2,size=(M,Ttotal))\n",
      "\n",
      "#contains weights for each particle at each time step\n",
      "w=ones((M,Ttotal))\n",
      "\n",
      "#normalize weights\n",
      "w=w/M\n",
      "\n",
      "k=0\n",
      "for t in range(1,Ttotal):\n",
      "    r1 = rand(M) # a 1 x M random prob matrix\n",
      "    for i in range(M):\n",
      "        if r1[i] < a:\n",
      "            xp[i,t] = 1-x[i,t-1] # change the predicted output\n",
      "            k=k+1\n",
      "        else:\n",
      "            xp[i,t] = x[i,t-1] # don't change the predicted output\n",
      "        # generate weight based on observation likelihood\n",
      "        if y[t] == xp[i,t]:\n",
      "            w[i,t] = b\n",
      "        else:\n",
      "            w[i,t] = 1-b\n",
      "    #normalize\n",
      "    w[:,t] = w[:,t] / sum(w[:,t])\n",
      "    \n",
      "    #generate M new particles\n",
      "    j=0\n",
      "    while j < M-1:\n",
      "        i = randint(M)\n",
      "        if rand() < w[i,t]:\n",
      "            x[j,t] = xp[i,t]\n",
      "            j = j+1\n",
      "\n",
      "# calculte the averaged predicted hidden state\n",
      "pred = zeros(Ttotal)\n",
      "for t in range(Ttotal):\n",
      "    pred[t] = (sum(xp[:,t])/M)\n",
      "\n",
      "#plot the results\n",
      "plot([i for i in range(Ttotal)], xs)\n",
      "ylim([-1,2])\n",
      "plot([i for i in range(Ttotal)], pred)\n",
      "legend(['actual hidden state', 'predicted hidden state'])\n",
      "xlabel('time')\n",
      "ylabel('mood')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}